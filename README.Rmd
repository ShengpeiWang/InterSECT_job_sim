---
title: "Exploring the Data Analytics Job Simulation Project"
output:
  html_document:
    df_print: paged
---

I was introduced to the job simulation resource to explore different careers. 
My interest is in data analytics, and here are the instructions for the relevant simulation:
https://intersectjobsims.com/library/using-data-to-make-predictions/

# Data cleanup 
Before I import the data into R, I first looked at all the data in excel, and I made a few changes:      
1. I noticed that the date column showed the data as date and time. But all the times were shown as 12am, so I reformatted the column to be date only.    
2. The phone number, address lines and postal code info was hard to compare, because different countries have different systems. And since the state and country codes provide location information, I just deleted those columns.     
3. I save the modifies data as a csv file for easier import

# Prep the environment
```{r}
library(tidyverse)
library(gridExtra)
library(lme4)
setwd("~/GitHub/InterSECT_job_sim")
```
# Import the data
```{r}
dat_sale <- read_csv("sales_data_sample-1.csv")
head(dat_sale)
dat_sale <- dat_sale %>% mutate (orderdate = parse_date(ORDERDATE, "%m/%d/%Y"))
```

# Data exploration
## Let's first look at the data distribution for some of the quantitative variables:
```{r}
phs <- hist(dat_sale$SALES)
```
Mostly normal, with single peak around 3000 and slightly skewed to the right.
```{r}
phq <- hist(dat_sale$QUANTITYORDERED, breaks = 40) 
```
Not normal at all, more like uniform distibution between 20-50 with some outliers
```{r}
phpr <- hist(dat_sale$PRICEEACH)
```
Not normal either, the bar of 100 is much higher than all others
```{r}
pbn <- ggplot(dat_sale, aes(ORDERLINENUMBER))+geom_bar()
pbn
```
Very strange, monotonic decrease of counts as orderline# increases.
This might be an variable that's added for the simulation only.
```{r}
pbd <- ggplot(dat_sale, aes(orderdate))+geom_bar()
pbd
```
Possibly some seasonal pattern, that there is more sale at the end of the season
**bold I think either the date or the quarter when the order is placed will be important**
```{r}
pby <- ggplot(dat_sale, aes(YEAR_ID))+geom_bar()
range(dat_sale$orderdate)
pby
```
More sales in 2004, but the 2015 data only extend to 2005-05-31
**So I will need to be careful when interpreting trends about different years**
```{r}
pbc <- ggplot(dat_sale, aes(PRODUCTLINE))+geom_bar()
pbc
```
A lot of classic cars and less trains
```{r}
pbcus <- ggplot(dat_sale, aes(CUSTOMERNAME))+geom_bar()
pbcus
```
Two outlier customers that made >150 orders
```{r}
pbcity <- ggplot(dat_sale, aes(CITY, color = COUNTRY))+geom_bar()
pbcity
```
Three cities make the most orders, and I the bars look awful like the previous graph. 
Let's see if the top customors are from the same place:
```{r}
dat_sale %>% group_by (CUSTOMERNAME) %>%
  summarize(orderN = n(), sale = mean(SALES))%>%
  ungroup() %>%
  summarize(median_order = median(orderN), median_sale = median(sale))
dat_sale %>% group_by (CUSTOMERNAME) %>% 
  summarize(orderN = n(), sale = mean(SALES), 
            city = sample(CITY,1), country = sample(COUNTRY,1)) %>%
  filter(orderN > 50 ) # look at only customers with more than 50 orders

```
It is clear that some of the customers are extreme in terms of the number of their orders, but theie order sizes are about average (~3500).
```{r}
pbdeal <- ggplot(dat_sale, aes(DEALSIZE))+geom_bar()
pbdeal
```
Most of the orders are small and medium. Compared to the previous table, it is clear that different customers are extreme in terms of order numbers and in terms of sales.

# Let's build a model
## Before I model anything, first check whether correlations are as expected:
```{r}
ggplot(dat_sale, aes(I(QUANTITYORDERED*PRICEEACH), SALES))+geom_point()
```
I was surprised to find that sales are different than the products of quantities and prices of each item. But that's good news for modeling. Since the points are spread out, I can have both factors in the same model. 
```{r}
ggplot(dat_sale, aes(SALES, color = DEALSIZE))+geom_bar()
```
Sale size is just a categorical division of SALES, so there two variables cannot be in the same model.
```{r}
dat_sale %>%
  group_by(MSRP,PRODUCTCODE) %>%
  summarise(n = n())
length(levels(as.factor(dat_sale$PRODUCTCODE)))
length(levels(as.factor(dat_sale$MSRP)))
```
So MSRP is nested in PRODUCTCODE... cannot have both of them in the same model
```{r}
dat_sale %>%
  group_by(ORDERNUMBER) %>%
  summarise(n = n(), a = mean(SALES), b = sd(SALES))
```
Why are order numbers not unique?
I guess I can use order number as a predictor variable then

Generate the appropriate subset of data:   
* If sales is important, we only need to focus on orders that are either processed or in process.
```{r}
data_model <- dat_sale %>% filter(STATUS %in% c("In Process", "Resolved", "Shipped")) %>%
  na.omit
```
## Let's plug everything into a model:
```{r}
FIT1 <- lmer(data = data_model, 
             SALES ~ scale(QUANTITYORDERED)*scale(PRICEEACH) + (ORDERLINENUMBER) + 
                     as.factor(QTR_ID) + as.factor(YEAR_ID) + 
                    (1|PRODUCTLINE) + (1|PRODUCTCODE) + (1|CUSTOMERNAME) + (1|CITY) + (1|STATE) + (1|COUNTRY) + (1|ORDERNUMBER)) 
summary(FIT1)
```
Sorry about this huge table.    
Let's first look at the random effects:    
* Only product code and product line had an effect. But from the previous data visualization, we know that cars sell better.   
* Custormer mame or location (city, state, country) have no effect.   
Next, let's look at the fixed effects:   
* QuantityOrdered and PriceEach had the most significant effects, highes t values.   
* Sales in the the fourth quarter was higher, but the effect may not be significant.     
    + (as.factor(QTR_ID)4) has a large estimate (55) but low t value (-2.04)   
* OrderlineNumber was somewhat important. 
* Year was not important.

## With the first model, we can slim down the model a bit:
First remove some of the extraneous fixed effects:
```{r}
FIT2 <- update(FIT1, .~. -as.factor(QTR_ID) - as.factor(YEAR_ID))
anova(FIT1, FIT2)
```
The two models are essentially the same in terms of predicative power. The reduced model (FIT2) is slightly better, because it contains less variables.   

Let's see if we can slim down the model further by removing random effects:
```{r}
FIT3 <- update(FIT2, .~. - (1|CUSTOMERNAME) - (1|CITY) - (1|STATE) - (1|COUNTRY) - (1|ORDERNUMBER))
anova(FIT2, FIT3)
```
Again the reduced model perform just as well. The simple fact of needing less data for the same power makes the last model superior.   

I'm a little curious whether excluding the canceled orders did anything:
```{r}
FITR <- lmer( data = dat_sale, 
             SALES ~ scale(QUANTITYORDERED)*scale(PRICEEACH) + ORDERLINENUMBER + 
                     (1|STATUS) +
                     (1|PRODUCTLINE) + (1|PRODUCTCODE))
summary(FITR)
```
STATUS is not important. 

*I would recommend FIT3 as the final model for future use*
Basically:    
SALES ~ scale(QUANTITYORDERED)*scale(PRICEEACH) + ORDERLINENUMBER + 
                     (1|PRODUCTLINE) + (1|PRODUCTCODE)

## PART 2. Which orders are more likely to be canceled. 
To be added
